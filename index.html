<html>
<head>
  <meta charset="utf-8"/>
  <!-- Material Design fonts -->
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Roboto:300,400,500,700">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/icon?family=Material+Icons">

  <!-- Bootstrap -->
  <!-- <link rel="stylesheet" type="text/css" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css"> -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  <!-- Bootstrap Material Design -->
  <link rel="stylesheet" type="text/css" href="dist/css/bootstrap-material-design.css">
  <link rel="stylesheet" type="text/css" href="dist/css/ripples.min.css">
  <style>
    body {
    	padding-top: 70px;
    	font-weight: normal;
    }
    th {
    	font-weight: bold;
    }
	th, td {
	    font-size: 14px;
	    padding: 10px;
	    border: solid 1px lightgrey;
	}
	.topic {

	}
	.reading {
		max-width: 450px;
	}
	.no-class {
		background-color: lightgrey;
	}
  </style>
  <title>AI599: Special Topics in Machine Learning - Deep Learning and Real-world Applications | Spring 2022</title>
</head>
<body>
	

<div class="navbar navbar-default navbar-fixed-top">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">AI599</a>
    </div>
    <div class="navbar-collapse collapse navbar-responsive-collapse">
      <ul class="nav navbar-nav">
        <li class="active"><a href="index.html">Home</a></li>
        <li><a href="reading-response.html">Reading Response</a></li>
        <li><a href="topic-presentation.html">Topic Presentation</a></li>
        <!-- <li><a href="assignments.html">Assignments</a></li> -->
<!--         <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Assignments <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="#">Assignment 1</a></li>
            <li><a href="#">Assignment 2</a></li>
            <li><a href="#">Assignment 3</a></li>
            <li><a href="#">Assignment 4</a></li>
          </ul>
        </li> -->
        <!-- <li><a href="design-project.html">Design Project</a></li> -->
        <!-- <li><a href="projects/index.html">Project Gallery</a></li> -->
        <li><a href="class-logistics.html">Class Logistics</a></li>
      </ul>
    </div>
  </div>
</div>


<div class="container-fluid main">
	<div class="row">
		<div class="col-md-12">
          <div class="well">
            <div>KAIST GSAI Spring 2022</div>
            <h1>AI599: Special Topics in Machine Learning : Deep Learning and Real-world Applications</h1>

            <p> Deep learning is now an integral part of daily systems and tools people use, and therefore no longer is a concern of only academic research. You will get the front-row experience on practical issues in research and development of deep learning systems from the leading experts and researchers. Major course activities include:</p>

            <ul>
              <li><strong>Reading Response</strong>: You'll read and discuss important papers and articles in the field. Each week, there will be 1-2 reading assignments, for which you'll write a short response to.</li>
              <li><strong>Topic Presentation</strong>: Once a semester, you'll lead the class by summarizing the readings, and spurring the in-class discussion. </li>
              <!-- <li><strong>Design Project</strong>: In a semester-long team project, you'll design, build, and test your own crowdsourcing / social computing system. If you have an ongoing research project that might benefit from having a crowdsourcing / social computing component, connecting to your research is encouraged. </li> -->
              <li><strong>In-class Activities</strong>: Each class will feature activities that will help you understand core concepts introduced in the course. </li>
            </ul>

          </div>
        <div id="source-button" class="btn btn-primary btn-xs" style="display: none;">&lt; &gt;</div></div>		
	</div>

	<div class="row">
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Course Staff</h3>
	            </div>
	            <div class="panel-body">
	              <strong>Instructors: </strong><br/>
                  &nbsp;&nbsp;&nbsp;&nbsp;<a href="http://minsukchang.com/">Prof. Minsuk Chang</a><br/>
                  &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://dongyoonhan.github.io/">Prof. Dongyoon Han</a><br/>
                  &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.sang-woo-lee.com/">Prof. Sangwoo Lee</a>
               <br/><br/>
	              <!-- &nbsp;&nbsp;&nbsp;&nbsp;<em>Office Hours</em>: 4-5pm Tue/Thu @ N1-605<br/><br/> -->
	              <strong>TAs: </strong></br>
                  &nbsp;&nbsp;&nbsp;&nbsp; <a href="">Sunghyun Park </a><br/>
                  &nbsp;&nbsp;&nbsp;&nbsp; <a href="">Dongmin Choi </a><br/><br/>
	              <!-- &nbsp;&nbsp;&nbsp;&nbsp;<em>Office Hours</em>: TBD <br/><br/> -->
                <strong>Staff Mailing List</strong>:<br/>&nbsp;&nbsp;&nbsp;&nbsp; <a href="mailto:dl_ai599@navercorp.com">dl_ai599@navercorp.com </a> <br/>
                &nbsp;&nbsp;&nbsp;&nbsp; note: this is a group email address that includes the instructors and the TAs. 
                 <!-- <a href="mailto:cs492@kixlab.org">cs492@kixlab.org</a> -->
	            </div>
	          </div>         
	        </div>	     
        </div>         
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Time &amp; Location</h3>
	            </div>
	            <div class="panel-body">
	              <strong>When</strong>: 10:30am-13:15pm Fri<br/>
	              <strong>Where</strong>:  
	            </div>
	          </div>         
	        </div>        
        </div> 
    
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Links</h3>
	            </div>
	            <div class="panel-body">
	              <strong>Course Website</strong>: <a href="https://ai599.github.io/spring-2022/">https://ai599.github.io/spring-2022/</a><br/>
	              <strong>Submission &amp; Grading</strong>: <a href="https://klms.kaist.ac.kr/course/view.php?id=136467">KLMS</a><br/>
	              <strong>Discussion Forum</strong>: TBD <a href=""></a>
	            </div>
	          </div>         
	        </div>        
        </div>
    </div>

	<div class="row">
		<div class="col-md-12">
	        <div class="bs-component">
	          <div class="panel panel-info">
	            <div class="panel-heading">
	              <h3 class="panel-title">Updates</h3>
	            </div>
	            <div class="panel-body">
	              <ul>
                  <li>3/18: The lecture slide for each class will be provided before the class at KLMS (If there are no restricted contents), and the recorded clips for each class are currently unavailable.
 </li>			      
                  <li>3/4: First day of class!  </li>
                  <li>3/3: Extra enrollment is closed <s>- but spaces might open up if others unregister. If you want to be waitlisted, please fill in <a href="">this survey</a>. We will enroll you first come first serve once spaces open up.</s></li>
                  <li><p style="color:red"><strong>3/2: You may "audit" or "sit in" this class, but you still have to submit reading responses and actively participate in class activities. If you're interested, please send an email to <a href="mailto:dl_ai599@navercorp.com">dl_ai599@navercorp.com </a>.</strong></p></li>
                  <li>3/2: We are accepting extra enrollments, but spaces are limited to total of 46 students. If you're interested in taking this class, please send an email to <a href="mailto:dl_ai599@navercorp.com">dl_ai599@navercorp.com </a> and fill in <a href="">this survey</a>. Current headcount: 46/46 </li>
	              	<li>2/28: Welcome to the deep learning and real-world applications class! We're still finalizing the schedule and the reading list. Stay tuned! </li>
	              </ul>
	            </div>
	          </div>         
	        </div>        
        </div>
    </div>


  <div class="row">
    <div class="col-md-12">
          <div class="bs-component">
            <div class="panel panel-primary">
              <div class="panel-heading">
                <h3 class="panel-title">Schedule</h3>
              </div>
              <div class="panel-body">
                <table class="table table-striped">
                  <tr>
                    <th>Week</th>
                    <th>Date</th>
                    <th class="topic">Topic</th>
                    <th class="topic">Invited Speaker</th>
                    <th class="reading">Reading <em><small>(<span class="label label-primary">response</span> indicates a reading response is required for one of the two articles.)</em></small></th>
                    <th>Due</th>
                  </tr>
                  <tr>
                    <td>1</td>
                    <td>3/4</td>
                    <td>Introduction & Course Overview + AI research in industry</td>
                    <td>ÌïòÏ†ïÏö∞</td>
                    <td> <strong> please read the updated course syllabus, and please ask any questions you might have. </strong></td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>3/11</td>
                    <td>Representation learning in computer vision <br/> 
                      Session 1: Learning representations with evolved model architectures
                      <br/></td>
                    <td>ÌïúÎèôÏú§</td>
                    <td> (1) <span class="label label-primary">response 1</span> Tan, Mingxing, and Quoc Le. <a href="https://arxiv.org/abs/2104.00298"> "EfficientNetV2: Smaller Models and Faster Training."</a>, ICML 2021 <br/>
                      (2) <span class="label label-primary">response 1</span> Liu, Ze, et al. <a href="https://arxiv.org/abs/2103.14030"> "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows."</a>, ICCV 2021 <br/><br/>
                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Han, Dongyoon, et al. <a href="https://arxiv.org/abs/2007.00992"> "Rethinking Channel Dimensions for Efficient Model Design."</a>, CVPR 2021 </li>
                        <li> Tolstikhin, Ilya O., et al. <a href="https://arxiv.org/abs/2105.01601"> "MLP-Mixer: An all-MLP Architecture for Vision."</a>, NeuIPS 2021 </li>
                        <li> Dosovitskiy, Alexey, et al. <a href="https://arxiv.org/abs/2010.11929"> "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale."</a>, ICLR 2021 </li>
                        <li> Heo, Byeongho, et al. <a href="https://arxiv.org/abs/2103.16302"> "Rethinking Spatial Dimensions of Vision Transformers."</a>, ICCV 2021 </li>
                        <li> Khan, Salman, et al. <a href="https://arxiv.org/abs/2101.01169"> "Transformers in Vision: A Survey."</a>, ACM Computing Surveys 2021</li>
                      </ul>                    
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>3/11</td>
                    <td>Representation learning in computer vision <br/>
                      Session 2: Practical scenarios and applications in computer vision<br/></td>
                    <td>Ïú†ÏòÅÏ§Ä</td>
                    <td> (1) <span class="label label-primary">response 2</span> An, Xiang, et al. <a href="https://arxiv.org/abs/2010.05222"> "Partial FC: Training 10 Million Identities on a Single Machine."</a>, ICCV 2021<br/>
                      (2) <span class="label label-primary">response 2</span> Sculley, David, et al. <a href="https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf"> "Hidden technical debt in machine learning systems."</a>, NeurIPS 2015 <br/><br/>
                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Guo, Yandong, et al. <a href="https://arxiv.org/abs/1607.08221"> "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition."</a>, ECCV 2016</li>
                        <li> Zhu, Zheng, et al. <a href="https://arxiv.org/abs/2103.04098"> "WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition."</a>, CVPR 2021</li>
                      </ul>                    
                    </td>
                    <td></td>
                  </tr>                
                  <tr>
                    <td>3</td>
                    <td>3/18</td>
                    <td>Towards reliable machine learning<br/>
                    Session 1: Definition and real examples of shortcut learning</td>
                    <td>Ï†ÑÏÉÅÌòÅ</td>
                    <td>(1) <span class="label label-primary">response 1</span> Brendel, et al. <a href="https://arxiv.org/abs/1904.00760"> "Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet."</a>, ICLR 2019   <br/>
                      (2) <span class="label label-primary">response 1</span> Geirhos, et al. <a href=" https://arxiv.org/abs/1811.12231"> "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness."</a>, ICLR 2019
                      <br/><br/>


                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Geirhos, et al. <a href="https://arxiv.org/abs/2004.07780"> "Shortcut Learning in Deep Neural Networks."</a>, Nature Machine Intelligence 2020. </li> 
                        <li> Scimeca, et al. <a href="https://arxiv.org/abs/2110.03095"> "Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective."</a>, ICLR 2022.</li> 
                        <li> de Vries, Terrance, et al. <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/de_Vries_Does_Object_Recognition_Work_for_Everyone_CVPRW_2019_paper.pdf"> "Does object recognition work for everyone?."</a> CVPR Workshops. 2019.</li> 
                    </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>3</td>
                    <td>3/18</td>
                    <td>Towards reliable machine learning<br/>
                    Session 2: Attempts to mitigate shortcut learning</td>
                    <td>Ï†ÑÏÉÅÌòÅ</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Madry, et al. <a href="https://arxiv.org/abs/1706.06083"> "Towards Deep Learning Models Resistant to Adversarial Attacks."</a>, ICLR 2018<br/>
                      (2) <span class="label label-primary">response 2</span> Ganin, et al. <a href="https://jmlr.org/papers/volume17/15-239/15-239.pdf"> "Domain-Adversarial Training of Neural Networks."</a>, JMLR 2016
                      <br/><br/>


                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Bahng, Hyojin, et al. <a href="https://arxiv.org/abs/1910.02806"> "Learning De-biased Representations with Biased Representations"</a>, ICML 2020</li>
                        <li> Nam, Junhyun, et al. <a href="https://arxiv.org/abs/2007.02561"> "Learning from Failure: Training Debiased Classifier from Biased Classifier"</a>, NeurIPS 2020</li>
                        <li> Cha, Junbum, et al. <a href="https://arxiv.org/abs/2102.08604"> "SWAD: Domain Generalization by Seeking Flat Minima"</a>, NeurIPS 2021</li>

                    </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>4</td>
                    <td>3/25</td>
                    <td>Multimodal representation learning<br/>
                    Session 1: Multimodal deep learning</td>
                    <td>ÍπÄÏßÑÌôî</td>
                    <td> 
                      (1) <span class="label label-primary">response 1</span> Kim, Jin-Hwa, et al. <a href="http://arxiv.org/abs/1805.07932"> "Bilinear Attention Networks."</a>, NeurIPS 2018
                      <br/> 
                      (2) <span class="label label-primary">response 1</span> Anderson, Peter, et al. <a href="http://arxiv.org/abs/1707.07998"> "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering."</a>, CVPR 2018<br/><br/>  

                      <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Ngiam, Jiquan, et al.<a href="https://people.csail.mit.edu/khosla/papers/icml2011_ngiam.pdf"> "Multimodal Deep Learning."</a>, ICML 2011</li>
                        <li> Goyal, et al. <a href="https://arxiv.org/abs/1612.00837"> "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering."</a>, CVPR 2017  </li>
                        <li> Hudson, Drew A., and Christopher D. Manning. <a href="http://arxiv.org/abs/1902.09506"> "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering."</a>, CVPR 2019</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>4</td>
                    <td>3/25</td>
                    <td>Multimodal representation learning <br/>
                    Session 2: Vision-and-Language Pre-training</td>
                    <td>ÍπÄÏõêÏû¨</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Lu, Jiasen, et al. <a href="https://arxiv.org/abs/1908.02265"> "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks."</a>, NeurIPS 2019 <br/>
                      (2) <span class="label label-primary">response 2</span> Kim, Wonjae, et al. <a href="https://arxiv.org/abs/2102.03334"> "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision."</a>, ICML 2021 <br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Chen, Yen-Chun, et al. <a href="https://arxiv.org/abs/1909.11740"> "UNITER: UNiversal Image-TExt Representation Learning."</a>, ECCV 2020</li>
                        <li> Singh, Amanpreet, et al. <a href="https://arxiv.org/abs/2112.04482"> "FLAVA: A Foundational Language And Vision Alignment Model."</a>, arXiv 2021</li>
                        <li> Li, Junnan, et al. <a href="https://arxiv.org/abs/2201.12086"> "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation."</a>, arXiv 2022</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>5</td>
                    <td>4/1</td>
                    <td>Noisy Labeling + <br/> Practical scenarios and applications in computer vision  <br/></td>
                    <td>ÏÜ°ÌôòÏ§Ä</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span> Han, Bo, et al. <a href="https://arxiv.org/abs/1804.06872"> "Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels."</a>, NeurIPS 2018
                   <br/>
                    (2) <span class="label label-primary">response 1</span> Li, Junnan, et al. <a href="https://arxiv.org/abs/2002.07394"> "DivideMix: Learning with Noisy Labels as Semi-supervised Learning."</a>, ICLR 2020 <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Zhang, Chiyuan, et al. <a href="https://arxiv.org/abs/1611.03530"> "Understanding deep learning requires rethinking generalization."</a>, ICLR 2017‚Äã </li>
                      <li> Jiang, Lu, et al. <a href="https://arxiv.org/abs/1712.05055"> "MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels."</a>, ICML 2018‚Äã</li>
                      <li> Song, Hwanjun, et al. <a href="http://proceedings.mlr.press/v97/song19b.html"> "SELFIE: Refurbishing unclean samples for robust deep learning."</a>, ICML 2019‚Äã</li>
                      <li> Song, Hwanjun, et al. <a href="https://arxiv.org/abs/2012.04337"> "Robust Learning by Self-Transition for Handling Noisy Labels."</a>, KDD 2021</li>
                      <li> Song, Hwanjun, et al. <a href="https://arxiv.org/abs/2007.08199"> "Learning from Noisy Labels with Deep Neural Networks: A Survey."</a>, TNNLS 2022</li>
                    </ul>
                  </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>5</td>
                    <td>4/1</td>
                    <td>Noisy Labeling + <br/> Practical scenarios and applications in computer vision  <br/></td>
                    <td> ÏúÑÎèôÏú§</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> Feichtenhofer, Christoph, et al. <a href="https://arxiv.org/abs/1812.03982"> "SlowFast Networks for Video Recognition."</a>, ICCV 2019
                    <br/>
                    (2) <span class="label label-primary">response 2</span> Wang, Xiaolong, et al. <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf"> "Non-local Neural Networks."</a>, CVPR 2018<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Carreira, Joao and Zisserman, Andrew. <a href="https://arxiv.org/abs/1705.07750"> "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset."</a>, CVPR 2017</li>
                      <li> Cu, Chunhui, et al. <a href="https://arxiv.org/abs/1705.08421"> "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions."</a>, CVPR 2018 </li>
                      <li> Kim, Jinhyung, et al. <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Regularization_on_Spatio-Temporally_Smoothed_Feature_for_Action_Recognition_CVPR_2020_paper.pdf"> "Regularization on Spatio-Temporally Smoothed Feature for Action Recognition."</a>, CVPR 2020</li>
                    </ul>
                  </td>
                    <td></td>
                  </tr>                  
                  <tr>
                    <td>6</td>
                    <td>4/8</td>
                    <td>Practical scenarios and applications in computer vision</td>
                    <td>Î∞±ÏòÅÎØº</td>
                  <td>
                    (1) <span class="label label-primary">response 1</span> Kittenplon, Yair, et al. <a href="https://arxiv.org/abs/2202.05508"> "Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer."</a>, arXiv 2022
                   <br/>
                    (2) <span class="label label-primary">response 1</span> Baek, Youngmin, et al. <a href="https://arxiv.org/abs/1904.01941"> "Character region awareness for text detection."</a>, CVPR 2019<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Baek, Youngmin, et al. <a href="https://arxiv.org/abs/1904.01906"> "What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis."</a>, ICCV 2019</li>
                      <li> Baek, Youngmin, et al. <a href="https://arxiv.org/abs/2007.09629"> "Character Region Attention For Text Spotting"</a>, ECCV 2020</li>
                      <li></li>
                    </ul>
                  </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>6</td>
                    <td>4/8</td>
                    <td>Practical scenarios and applications in computer vision</td>
                    <td>Ïù¥Î∞îÎèÑ</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> Cha, Junbum, et al. <a href="https://arxiv.org/abs/2005.10510"> "Few-shot Compositional Font Generation with Dual Memory."</a>, ECCV 2020
                    <br/>
                    (2) <span class="label label-primary">response 2</span> Park, Song, et al. <a href="https://arxiv.org/abs/2009.11042"> "Few-shot Font Generation with Localized Style Representations and Factorization."</a>, AAAI 2021 <br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Park, Song, et al. <a href="https://arxiv.org/abs/2104.00887"> "Multiple Heads are Better than One:Few-shot Font Generation with Multiple Localized Experts."</a>, ICCV 2021</li>
                      <!-- <li></li>
                      <li></li> -->
                    </ul>
                  </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>7</td>
                    <td>4/15</td>
                    <td>Generative models</td>
                    <td>ÍπÄÏú§ÏßÄ </td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Ji, Xu, et al. <a href="https://arxiv.org/abs/1807.06653"> "Invariant Information Clustering for Unsupervised Image Classification and Segmentation."</a>, ICCV 2019<br/>
                      (2) <span class="label label-primary">response 1</span> Van, Gansbeke, et al. <a href="https://arxiv.org/abs/2005.12320"> "SCAN: Learning to Classify Images without Labels."</a>, ECCV 2020<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Chen, Xi, et al. <a href="https://arxiv.org/abs/1606.03657"> "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets."</a>, NeurIPS 2016</li>
                        <li> Krishna, Kumar, Singh, et al. <a href="https://arxiv.org/abs/1811.11155"> "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery."</a>, CVPR 2019</li>
                        <li> Kim, Yunji and Ha, Jung-Woo. <a href="https://arxiv.org/abs/2112.14971"> "Contrastive Fine-grained Class Clustering via Generative Adversarial Networks."</a>, ICLR 2022</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>  
                  <tr>
                    <td>7</td>
                    <td>4/15</td>
                    <td>Generative models</td>
                    <td> ÍπÄÏ§ÄÌò∏ </td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Kang, Minguk and Park, Jaesik. <a href="https://arxiv.org/abs/2006.12681"> "ContraGAN: Contrastive Learning for Conditional Image Generation."</a> NeurIPS 2020<br/>
                      (2) <span class="label label-primary">response 2</span> Liu, Bingchen, et al. <a href="https://arxiv.org/abs/2101.04775"> "Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis."</a> ICLR 2021<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Zhao, Long, et al. <a href="https://arxiv.org/abs/2106.07631"> "Improved Transformer for High-Resolution GANs."</a>, ICLR 2021</li>
                        <li> Karras, Tero, et al. <a href="https://arxiv.org/abs/1912.04958"> "Analyzing and Improving the Image Quality of StyleGAN."</a>, CVPR 2020</li>
                        <li> Zhang, Han, et al. <a href="https://arxiv.org/abs/1910.12027"> "Consistency Regularization for Generative Adversarial Networks."</a>, ICLR 2020</li>
                        <li> Kim, Junho, et al. <a href="https://arxiv.org/abs/2112.04120"> "Feature Statistics Mixing Regularization for Generative Adversarial Networks."</a>, arXiv 2021</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>                    
                  <tr>
                    <td>8</td>
                    <td>4/22</td>
                    <td class="no-class">No Class (Midterm exams)</td>
                    <td class="no-class"></td>
                    <td></td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>9</td>
                    <td>4/29</td>
                    <td>Voice synthesis and applications  <br/>
                    <td>ÏÜ°ÏùÄÏö∞</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Shen, Jonathan, et al. <a href="https://arxiv.org/abs/1712.05884"> "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions."</a>, ICASSP 2018
                      <br/>
                      (2) <span class="label label-primary">response 1</span> Ren, Yi, et al. <a href="https://arxiv.org/abs/2006.04558"> "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech."</a>, ICLR 2021<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Wang, Yuxuan, et al. <a href="https://arxiv.org/abs/1703.10135"> "Tacotron: Towards End-to-End Speech Synthesis."</a>, Interspeech 2017</li>
                        <li> Li, Naihan, et al. <a href="https://arxiv.org/abs/1809.08895"> "Neural Speech Synthesis with Transformer Network."</a>, AAAI 2019</li>
                        <li> Yi, Ren, et al. <a href="https://arxiv.org/abs/1905.09263"> "FastSpeech: Fast, Robust and Controllable Text to Speech."</a>, NeurIPS 2019</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <td>9</td>
                  <td>4/29</td>
                  <td>Voice synthesis and applications  <br/>
                  <td>Ìô©ÎØºÏ†ú</td>
                  <td>
                    (1) <span class="label label-primary">response 2</span> Kumar, Kundan, et al. <a href="https://arxiv.org/abs/1910.06711"> "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis."</a>, NeurIPS 2019<br/>
                    (2) <span class="label label-primary">response 2</span> Yamamoto, Ryuichi, et al. <a href="https://arxiv.org/abs/1910.11480"> "Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram."</a>, ICASSP 2020<br/><br/>
                  
                  <strong>Recommended reading</strong> <br/>
                    <ul>
                      <li> Oord, Aaron van den, et al. <a href="https://arxiv.org/abs/1609.03499"> "WaveNet: A Generative Model for Raw Audio."</a>, arXiv 2016</li>
                      <li> Oord, Aaron van den, et al. <a href="https://arxiv.org/abs/1711.10433"> "Parallel WaveNet: Fast High-Fidelity Speech Synthesis."</a>, ICML 2018</li>
                      <li> Kong, Jungil, et al. <a href="https://arxiv.org/abs/2010.05646"> "HiFi-GAN- Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis."</a>, NeurIPS 2020</li>
                    </ul>
                  </td>
                  <td></td>
                </tr>
                  <tr>
                    <td>10</td>
                    <td>5/6</td>
                    <td>Speech recognition and applications</td>
                    <td>ÍπÄÌïúÍ∑ú</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Hsu, Wei-Ning, et al. <a href="https://arxiv.org/abs/2106.07447"> "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units."</a>, IEEE/ACM Transactions on Audio, Speech, and Language Processing 2021<br/>
                      (2) <span class="label label-primary">response 1</span> Chung, Yu-An, et al. <a href="https://arxiv.org/abs/2108.06209"> "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training."</a>, arXiv 2021<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Baevski, Alexei, et al. <a href="https://arxiv.org/abs/2006.11477"> "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations."</a>, NeurIPS 2020</li>
                        <li> Zoph, Barret, et al. <a href="https://proceedings.neurips.cc/paper/2020/file/27e9661e033a73a6ad8cefcde965c54d-Paper.pdf"> "Self-training and Pre-training are Complementary for Speech Recognition."</a>, NeurIPS 2020</li>
                        <li> Zhang, Yu, et al. <a href="https://arxiv.org/abs/2010.10504"> "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition."</a>, NeurIPS 2020 Workshop‚Äã</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>10</td>
                    <td>5/6</td>
                    <td>Speech recognition and applications</td>
                    <td>Ï†ïÎÇ®Í∑ú</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Culati, Anmol, et al. <a href="https://arxiv.org/abs/2005.08100"> "Conformer: Convolution-augmented Transformer for Speech Recognition."</a>, Interspeech 2020<br/>
                      (2) <span class="label label-primary">response 2</span> Han, Wei, et al. <a href="https://arxiv.org/abs/2005.03191"> "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context."</a>, Interspeech 2020<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Graves, Alex, et al. <a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf"> "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks."</a>, ICML 2006</li>
                        <li> Amodei, Dario, et al. <a href="https://arxiv.org/abs/1512.02595"> "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin."</a>, ICML 2016</li>
                        <li> Graves, Alex. <a href="https://arxiv.org/abs/1211.3711"> "Sequence Transduction with Recurrent Neural Networks."</a>, ICML 2012 Workshop</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>                  
                  <tr>
                    <td>11</td>
                    <td>5/13</td>
                    <td>AutoML and Practical MLOps  <br/>
                    <td>ÍπÄÏßÄÌõà</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Real, Esteban, et al. <a href="https://arxiv.org/abs/2003.03384"> "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch."</a>, ICML 2020<br/>
                      (2) <span class="label label-primary">response 1</span> Falkner, Stefan, et al. <a href="https://arxiv.org/abs/1807.01774"> "BOHB: Robust and Efficient Hyperparameter Optimization at Scale."</a>, ICML 2018<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        Neural Architecture Search:
                        <li> Liu, Hanxiao, et al. <a href="https://arxiv.org/abs/1806.09055"> "DARTS: Differentiable Architecture Search."</a>, ICLR 2019</li>
                        <li> Dong, XuanYi and Yang, Yi. <a href="https://arxiv.org/abs/2001.00326"> "NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search."</a>, ICLR 2020</li>
                        Hyperparameter Optimization:
                        <li> Li, Lisha, et al. <a href="https://arxiv.org/abs/1603.06560"> "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization."</a>, JLMR 2018</li>
                        <li> Jaderberg, Max, et al. <a href="https://arxiv.org/abs/1711.09846"> "Population Based Training of Neural Networks."</a>, arXiv 2017</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>11</td>
                    <td>5/13</td>
                    <td>AutoML and Practical MLOps  <br/>
                    <td>ÏÑúÎèôÌïÑ</td>
                    <td>
                      No reading this week
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>12</td>
                    <td>5/20</td>
                    <td>NLP, Dialogues, and QA<br/></td>
                    <td>Ïù¥ÏÉÅÏö∞</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Devlin, et al. <a href="https://arxiv.org/abs/1810.04805"> "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding."</a>, NAACL 2019. 
                      <br/>
                      (2) <span class="label label-primary">response 1</span> Raffel, et al. <a href="https://arxiv.org/abs/1910.10683"> "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer."</a>, JMLR 2020. <br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Radford, Alec, et al. <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"> "Language Models are Unsupervised Multitask Learners."</a>, OpenAI 2019</li>
                        <li> Yoo, Kangmin, et al. <a href="https://arxiv.org/abs/2104.08826"> "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation."</a>, ACL Findings 2021</li>
                        <li> Kim, Sungdong, et al. <a href="https://arxiv.org/abs/2105.14454"> "NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation."</a>, , ACL 2021</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>12</td>
                    <td>5/20</td>
                    <td>NLP, Dialogues, and QA<br/></td>
                    <td>ÍπÄÏÑ±Îèô</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Roller, Stephen, et al. <a href="https://arxiv.org/abs/2004.13637"> "Recipes for building an open-domain chatbot."</a>, EACL 2021<br/>
                      (2) <span class="label label-primary">response 2</span> Lewis, Patrick, et al. <a href="https://arxiv.org/abs/2005.11401"> "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"</a>, NeurIPS 2020<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Izacard and Grave. <a href="https://arxiv.org/abs/2007.01282"> "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering."</a>, EACL 2021</li>
                        <li> Shuster, Kurt, et al. <a href="https://arxiv.org/abs/2104.07567"> "Retrieval Augmentation Reduces Hallucination in Conversation."</a>, EMNLP Findings 2021</li>
                        <li> Xu, Jing, et al. <a href="https://arxiv.org/abs/2107.07567"> "Beyond Goldfish Memory: Long-Term Open-Domain Conversation."</a>, arXiv 2021</li>
                        <li> Borgeaud, Sebastian, et al. <a href="https://arxiv.org/abs/2112.04426"> "Improving language models by retrieving from trillions of tokens."</a>, arXiv 2021</li>
                        <li> Sungdong, Kim and Gangwoo, Kim. <a href="https://arxiv.org/abs/2202.07280"> "Saving Dense Retriever from Shortcut Dependency in Conversational Search."</a>, arXiv 2022</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>                  
                  <tr>
                    <td>13</td>
                    <td>5/27</td>
                    <td>Hyperscale LM & NLP applications</td>
                    <td>Ïù¥Í∏∞Ï∞Ω</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Brown, et al. <a href="https://arxiv.org/abs/2005.14165"> "Language Models are Few-Shot Learners."</a>, NeurIPS 2021<br/>
                      (2) <span class="label label-primary">response 1</span> Rae, et al. <a href="https://arxiv.org/abs/2112.11446"> "Scaling Language Models: Methods, Analysis & Insights from Training Gopher."</a>, arXiv 2021.  <br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Smith, et al. <a href="https://arxiv.org/abs/2201.11990"> "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model."</a>, arXiv 2022</li>
                        <li> Tay, et al., <a href="https://arxiv.org/abs/2109.10686"> "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers."</a>, ICLR 2022</li>
                        <li>Kim, Boseop, et al. <a href="https://arxiv.org/abs/2109.04650"> "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers."</a>, EMNLP 2021</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>13</td>
                    <td>5/27</td>
                    <td>Hyperscale LM & NLP applications</td>
                    <td>Ïú†Í∞ïÎØº</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Lester, Brian, et al. <a href="https://arxiv.org/abs/2104.08691"> "The Power of Scale for Parameter-Efficient Prompt Tuning."</a>, EMNLP 2021<br/>
                      (2) <span class="label label-primary">response 2</span> Li, Xiang Lisa, and Percy, Liang. <a href="https://arxiv.org/abs/2101.00190"> "Prefix-Tuning: Optimizing Continuous Prompts for Generation."</a>, arXiv 2021<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> He, Junxian, et al. <a href="https://arxiv.org/abs/2110.04366"> "Towards a Unified View of Parameter-Efficient Transfer Learning."</a>, ICLR 2022</li>
                        <li> J. Hu, Edward, et al. <a href="https://arxiv.org/abs/2106.09685"> "LoRA: Low-Rank Adaptation of Large Language Models."</a>, arXiv 2021</li>
                        <li> Schick, Timo and Sch√ºtze, Hinrich <a href="https://arxiv.org/abs/2009.07118"> "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners."</a>, NAACL 2021</li>
                        <li> Ouyang, Long, et al. <a href="https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf"> "Training language models to follow instructions with human feedback (InstructGPT)</a>, OpenAI Blog 2022</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>                  
                  <tr>
                    <td>14</td>
                    <td>6/3</td>
                    <td>Human-centric NLP</td>
                    <td>Ïù¥ÌôîÎûÄ</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Dinan, Emily, et al. <a href="https://arxiv.org/abs/1911.03842"> "Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation."</a>, EMNLP 2020<br/>
                      (2) <span class="label label-primary">response 1</span> Perez, Ethan, et al. <a href="https://arxiv.org/abs/2202.03286"> "Red Teaming Language Models with Language Models."</a>, arXiv 2022.<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Bender, Emily M., et al. <a href="https://dl.acm.org/doi/10.1145/3442188.3445922"> "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ü¶ú."</a>, ACM Conference on Fairness, Accountability, and Transparency 2021</li>
                        <li> Liu, Haochen, et al. <a href="https://arxiv.org/abs/1910.10486"> "Does Gender Matter? Towards Fairness in Dialogue Systems."</a>, COLING 2020</li>
                        <li> Liu, Haochen, et al. <a href="https://arxiv.org/abs/2009.13028"> "Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning."</a>, EMNLP 2020</li>
                        <li> Sheng, Emily, et al. <a href="https://arxiv.org/abs/2010.12820"> "‚ÄúNice Try, Kiddo‚Äù: Investigating Ad Hominems in Dialogue Responses."</a>, NAACL 2021</li>
                        <li> Ma, Xinyao, et al. <a href="https://arxiv.org/abs/2010.13816"> "PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction."</a>, EMNLP 2020</li>
                        <li> Xu, Albert, et al. <a href="https://arxiv.org/abs/2104.06390"> "Detoxifying Language Models Risks Marginalizing Minority Voices."</a>, NAACL 2021</li>
                        <li> OpenAI. <a href="https://arxiv.org/abs/2112.09332"> "WebGPT: Improving the Factual Accuracy of Language Models through Web Browsing (WebGPT)."</a>,ArXiv 2021</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>14</td>
                    <td>6/3</td>
                    <td>Human-centric NLP</td>
                    <td>Ï†ïÏ§ÄÏòÅ, Ïù¥ÎØºÏïÑ</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Chung, JJY, et al. <a href="https://johnr0.github.io/assets/publications/CHI2022-TaleBrush.pdf">"TaleBrush: Sketching Stories with Generative Pretrained Language Models."</a>, CHI 2022 <br/>
                      (2) <span class="label label-primary">response 2</span> Lee, Mina, Percy Liang, and Qian Yang.<a href="https://arxiv.org/pdf/2201.06796"> "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities."</a>, CHI 2022 <br/><br/>
                      
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li>Clark, Elizabeth, et al. <a href="https://dl.acm.org/doi/10.1145/3172944.3172983">"Creative writing with a machine in the loop: Case studies on slogans and stories."</a>, IUI 2018</li>
                        <li> Singh, Nikhil, et al. <a href="https://dl.acm.org/doi/10.1145/3511599">"Where to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal Machine Intelligence."</a>, ToCHI 2022</li>
                        <li>Krause, Ben, et al. <a href="https://arxiv.org/abs/2009.06367">"Gedi: Generative discriminator guided sequence generation."</a>, EMNLP findings 2021</li>
                        <li>Qian, Jing et al. <a href="https://arxiv.org/abs/2202.13257">"Controllable Natural Language Generation with Contrastive Prefixes."</a>, arXiv 2022</li>
                        <li>Buschek, Daniel, Martin Z√ºrn, and Malin Eiband.<a href="https://arxiv.org/pdf/2101.09157.pdf"> "The impact of multiple parallel phrase suggestions on email input and composition behaviour of native and non-native english writers."</a>, CHI 2021</li>
                        <li>Calderwood, Alex, et al.<a href="https://www.cs.columbia.edu/~chilton/web/my_publications/Calderwood_How_Novelists_Use_Generative_Language_Models.pdf"> "How Novelists Use Generative Language Models: An Exploratory User Study."</a>, HAI-GEN+ user2agent@ IUI. 2020.</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>                  
                  <tr>
                    <td>15</td>
                    <td>6/10</td>
                    <td>Large-scale user modeling and its applications</td>
                    <td>Í≥ΩÌïòÎÖπ</td>
                    <td>
                      (1) <span class="label label-primary">response 1</span> Shin, et al. <a href="https://arxiv.org/abs/2111.11294"> "Scaling Law for Recommendation Models: Towards General-purpose User Representations"</a>, arXiv 2021<br/>
                      (2) <span class="label label-primary">response 1</span> Shin, et al. <a href="https://arxiv.org/abs/2106.00573"> "One4all user representation for recommender systems in e-commerce"</a>, arXiv 2021<br/><br/>
                    
                    <!-- <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li></li>
                        <li></li>
                        <li></li>
                      </ul> -->
                    </td>
                    <td></td>
                  </tr> 
                  <tr>
                    <td>15</td>
                    <td>6/10</td>
                    <td>Large-scale user modeling and its applications</td>
                    <td> Ï†ïÏßÄÏàò</td>
                    <td>
                      (1) <span class="label label-primary">response 2</span> Hsieh, et al. <a href="https://dl.acm.org/doi/10.1145/3038912.3052639"> "Collaborative Metric Learning."</a>, WWW 2017<br/>
                      (2) <span class="label label-primary">response 2</span> Kim, Boseop, et al. <a href="https://arxiv.org/abs/2109.04650"> "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers."</a>, EMNLP 2021<br/><br/>
                    
                    <strong>Recommended reading</strong> <br/>
                      <ul>
                        <li> Tran, et al. <a href="https://arxiv.org/abs/2108.04655"> "Hierarchical Latent Relation Modeling for Collaborative Metric Learning."</a>, RecSys 2021</li>
                        <li> OpenAI. <a href="https://openai.com/blog/gpt-3-apps/"> "GPT-3 Powers the Next Generation of Apps."</a>, OpenAI Blog 2021</li>
                        <li> Eric, Verduzco. <a href="https://nogood.io/2021/06/25/gpt-3-tools/"> "Best GPT-3 Tools, Examples and Use Cases."</a>, 2021</li>
                      </ul>
                    </td>
                    <td></td>
                  </tr>                   
                  <tr>
                    <td>16</td>
                    <td>6/17</td>
                    <td class='no-class'>No Class (Final exams)</td>
                    <td class="no-class"></td>
                    <td></td>
                    <td></td>
                  </tr> 
                                                                                                                                              
                </table>
              </div>
            </div>         
          </div>        
        </div>
    </div>                  

	<div class="row">
    <div class="col-md-4">
          <div class="bs-component">
            <div class="panel panel-primary">
              <div class="panel-heading">
                <h3 class="panel-title">Topics (tentative)</h3>
              </div>
              <div class="panel-body">
                Major topics include:
                <ul>
                 <li>Representation Learning</li>
                 <li>Reliable ML</li>
                 <li>Voice and Speech</li>
                 <li>NLP</li>
                 <li>MLOps</li>
                 <li>Recommendation systems</li>
                </ul>
              </div>
            </div>
          </div>
        </div>
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Grading</h3>
	            </div>
	            <div class="panel-body">
	            <ul>
                <li>Attendance: 20%</li>
      					<li>Reading responses: 40%</li>
      					<li>Topic presentation: 20%</li>
      					<li>Class participation: 10%</li>
                <li>Quizes: 10%</li>
				      </ul>

      				<strong>Late policy</strong>: Three lowest reading response grades will be removed. Each quiz score will be normalized identical to the score of 5 questions asked duing each class. No late submissions are allowed for the reading responses.
	            </div>
	          </div>         
	        </div>	        
        </div>
		<div class="col-md-4">
	        <div class="bs-component">
	          <div class="panel panel-primary">
	            <div class="panel-heading">
	              <h3 class="panel-title">Prerequisites</h3>
	            </div>
	            <div class="panel-body">
	            There are no official course prerequisites. But assignments involve a lot of reading, research experience in machine learning is useful, but not required. 
	            </div>
	          </div>         
	        </div>	        
        </div>   
    </div>

</div>

<script src="//code.jquery.com/jquery-1.10.2.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="dist/js/ripples.min.js"></script>
<script src="dist/js/material.min.js"></script>
<script>
  $(function () {
    $.material.init();
  });
</script>

</body>
</html>
